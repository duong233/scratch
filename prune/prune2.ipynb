{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning: A complete guide for beginners\n",
    "\n",
    "Goals\n",
    "In this assignment, you will practice pruning a classical neural network model to reduce both model size and latency. The goals of this assignment are as follows:\n",
    "\n",
    "* Understand the basic concept of pruning\n",
    "* Implement and apply fine-grained pruning\n",
    "* Implement and apply channel pruning\n",
    "* Get a basic understanding of performance improvement (such as speedup) from pruning\n",
    "* Understand the differences and tradeoffs between these pruning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchprofile\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torchprofile) (1.26.0)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torchprofile) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.4 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torchprofile) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torch>=1.4->torchprofile) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torchvision>=0.4->torchprofile) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from torchvision>=0.4->torchprofile) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin#\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision>=0.4->torchprofile) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from requests->torchvision>=0.4->torchprofile) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from requests->torchvision>=0.4->torchprofile) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from requests->torchvision>=0.4->torchprofile) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin#\\.conda\\envs\\duongbkcs\\lib\\site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\n",
      "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: torchprofile\n",
      "Successfully installed torchprofile-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torchprofile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23301aba090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_url(url, model_dir='.', overwrite=False):\n",
    "    import os, sys\n",
    "    from urllib.request import urlretrieve\n",
    "    target_dir = url.split('/')[-1]\n",
    "    model_dir = os.path.expanduser(model_dir)\n",
    "    # try:\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_dir = os.path.join(model_dir, target_dir)\n",
    "    cached_file = model_dir\n",
    "    if not os.path.exists(cached_file) or overwrite:\n",
    "        sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "        urlretrieve(url, cached_file)\n",
    "        return cached_file\n",
    "    # except Exception as e:\n",
    "    #     # remove lock file so download can be executed next time.\n",
    "    #     os.remove(os.path.join(model_dir, 'download.lock'))\n",
    "    #     sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    ARCH = [64,128,'M',256,256,'M',512,512,'M',512,512,'M']\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        layers = []\n",
    "        counts = defaultdict(int)\n",
    "        \n",
    "        def add(name: str, layer: nn.Module) -> None:\n",
    "            layers.append((f\"{name}{counts[name]}\", layer))\n",
    "            counts[name] += 1\n",
    "        \n",
    "        in_channels = 3\n",
    "        for x in self.ARCH:\n",
    "            if x != 'M':\n",
    "                add('conv', nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "                add('batchnorm', nn.BatchNorm2d(x))\n",
    "                add('relu', nn.ReLU(True))\n",
    "            else:\n",
    "                add('pool', nn.MaxPool2d(2))\n",
    "        \n",
    "        self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "        x = x.mean([2,3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        scheduler: LambdaLR,\n",
    "        callbacks = None\n",
    ") -> None:\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc='train'):\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # lan truyen nguoc\n",
    "        loss.backward()\n",
    "\n",
    "        # update optimizer, learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        model: nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        verbose=True\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc='eval'):\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # convert output to class\\\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "\n",
    "        num_samples += targets.size(0)\n",
    "        num_correct += (outputs==targets).sum()\n",
    "\n",
    "    return (num_correct/num_samples*100).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)\n",
    "\n",
    "\n",
    "def get_sparsity(tensor: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    calculate the sparsity of the given tensor\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    \"\"\"\n",
    "    return 1 - float(tensor.count_nonzero()) / tensor.numel()\n",
    "\n",
    "\n",
    "def get_model_sparsity(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    calculate the sparsity of the given model\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    \"\"\"\n",
    "    num_nonzeros, num_elements = 0, 0\n",
    "    for param in model.parameters():\n",
    "        num_nonzeros += param.count_nonzero()\n",
    "        num_elements += param.numel()\n",
    "    return 1 - float(num_nonzeros) / num_elements\n",
    "\n",
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "\n",
    "def get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fine_grained_prune(\n",
    "    test_tensor=torch.tensor([[-0.46, -0.40, 0.39, 0.19, 0.37],\n",
    "                              [0.00, 0.40, 0.17, -0.15, 0.16],\n",
    "                              [-0.20, -0.23, 0.36, 0.25, 0.03],\n",
    "                              [0.24, 0.41, 0.07, 0.13, -0.15],\n",
    "                              [0.48, -0.09, -0.36, 0.12, 0.45]]),\n",
    "    test_mask=torch.tensor([[True, True, False, False, False],\n",
    "                            [False, True, False, False, False],\n",
    "                            [False, False, False, False, False],\n",
    "                            [False, True, False, False, False],\n",
    "                            [True, False, False, False, True]]),\n",
    "    target_sparsity=0.75, target_nonzeros=None):\n",
    "    def plot_matrix(tensor, ax, title):\n",
    "        ax.imshow(tensor.cpu().numpy() == 0, vmin=0, vmax=1, cmap='tab20c')\n",
    "        ax.set_title(title)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        for i in range(tensor.shape[1]):\n",
    "            for j in range(tensor.shape[0]):\n",
    "                text = ax.text(j, i, f'{tensor[i, j].item():.2f}',\n",
    "                                ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "    test_tensor = test_tensor.clone()\n",
    "    fig, axes = plt.subplots(1,2, figsize=(6, 10))\n",
    "    ax_left, ax_right = axes.ravel()\n",
    "    plot_matrix(test_tensor, ax_left, 'dense tensor')\n",
    "\n",
    "    sparsity_before_pruning = get_sparsity(test_tensor)\n",
    "    mask = fine_grained_prune(test_tensor, target_sparsity)\n",
    "    sparsity_after_pruning = get_sparsity(test_tensor)\n",
    "    sparsity_of_mask = get_sparsity(mask)\n",
    "\n",
    "    plot_matrix(test_tensor, ax_right, 'sparse tensor')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('* Test fine_grained_prune()')\n",
    "    print(f'    target sparsity: {target_sparsity:.2f}')\n",
    "    print(f'        sparsity before pruning: {sparsity_before_pruning:.2f}')\n",
    "    print(f'        sparsity after pruning: {sparsity_after_pruning:.2f}')\n",
    "    print(f'        sparsity of pruning mask: {sparsity_of_mask:.2f}')\n",
    "\n",
    "    if target_nonzeros is None:\n",
    "        if test_mask.equal(mask):\n",
    "            print('* Test passed.')\n",
    "        else:\n",
    "            print('* Test failed.')\n",
    "    else:\n",
    "        if mask.count_nonzero() == target_nonzeros:\n",
    "            print('* Test passed.')\n",
    "        else:\n",
    "            print('* Test failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained and CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_url = \"https://hanlab.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
    "checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
    "model = VGG().cuda()\n",
    "print(f\"=> loading checkpoint '{checkpoint_url}'\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "recover_model = lambda: model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:38<00:00, 1724360.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar10\\cifar-10-python.tar.gz to data/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model_accuracy = evaluate(model, dataloader['test'])\n",
    "dense_model_size = get_model_size(model)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained Prune\n",
    "\n",
    "In this section, we will implement and perform fine-grained pruning.\n",
    "\n",
    "Fine-grained pruning removes the synapses with lowest importance. The weight tensor $W$ will become sparse after fine-grained pruning, which can be described with **sparsity**:\n",
    "\n",
    "> $\\mathrm{sparsity} := \\#\\mathrm{Zeros} / \\#W = 1 - \\#\\mathrm{Nonzeros} / \\#W$\n",
    "\n",
    "where $\\#W$ is the number of elements in $W$.\n",
    "\n",
    "In practice, given the target sparsity $s$, the weight tensor $W$ is multiplied with a binary mask $M$ to disregard removed weight:\n",
    "\n",
    "> $v_{\\mathrm{thr}} = \\texttt{kthvalue}(Importance, \\#W \\cdot s)$\n",
    ">\n",
    "> $M = Importance > v_{\\mathrm{thr}}$\n",
    ">\n",
    "> $W = W \\cdot M$\n",
    "\n",
    "where $Importance$ is importance tensor with the same shape of $W$, $\\texttt{kthvalue}(X, k)$ finds the $k$-th smallest value of tensor $X$, $v_{\\mathrm{thr}}$ is the threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In step 1, we calculate the number of zeros (num_zeros) after pruning. Note that num_zeros should be an integer. You could use either round() or int() to convert a floating number into an integer. Here we use round().\n",
    "* In step 2, we calculate the importance of weight tensor. Pytorch provides torch.abs(), torch.Tensor.abs(), torch.Tensor.abs_() APIs.\n",
    "* In step 3, we calculate the pruning threshold so that all synapses with importance smaller than threshold will be removed. Pytorch provides torch.kthvalue(), torch.Tensor.kthvalue(), torch.topk() APIs.\n",
    "* In step 4, we calculate the pruning mask based on the threshold. 1 in the mask indicates the synapse will be kept, and 0 in the mask indicates the synapse will be removed. mask = importance > threshold. Pytorch provides torch.gt() API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_grained_prune(tensor: torch.Tensor, sparsity: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param tensor: weight of layer\n",
    "    :param sparsity: float, pruing sparsity\n",
    "    \"\"\"\n",
    "\n",
    "    sparsity = min(max(0.0, sparsity), 1.0)\n",
    "\n",
    "    if sparsity == 1.0:\n",
    "        tensor.zero_()\n",
    "        return torch.zeros_like(tensor)\n",
    "    elif sparsity == 0.0:\n",
    "        return torch.ones_like(tensor)\n",
    "    \n",
    "    num_elements = tensor.numel()\n",
    "\n",
    "    #step 1: calculate zeros\n",
    "    num_zeros = round(sparsity * num_elements)\n",
    "    #step 2: calculate importance of weight\n",
    "    importance = torch.abs(tensor)\n",
    "    #step 3: calculate threshold\n",
    "    threshold, _ = torch.kthvalue(importance.flatten(), num_zeros)\n",
    "    #step 4: get binary mask\n",
    "    mask = importance > threshold\n",
    "    print('mask: ' + mask)\n",
    "\n",
    "    #step 5; apply mask to prune\n",
    "    tensor.mul(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineGrainedPrune:\n",
    "    def __init__(self) -> None:\n",
    "        self.mask = FineGrainedPrune.prune()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model):\n",
    "        for name, param in model.parameters():\n",
    "            if name in self.masks:\n",
    "                param *= self.masks[name]\n",
    "\n",
    "    def prune(self, model, sparsity_dict):\n",
    "        masks = dict()\n",
    "        for name, param in model.parameters():\n",
    "            if param.dim() > 1: #prune conv and fc weight\n",
    "                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DuongBKCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
